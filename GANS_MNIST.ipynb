{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANS MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tjqYbd-FmpOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "il_gA4bemt7-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "chMxbo4_oc-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "890703c6-c150-4d36-fc99-7c7184ab29a1"
      },
      "cell_type": "code",
      "source": [
        "mnist=input_data.read_data_sets('/content',one_hot=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-8d3cc5232d0e>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /content/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /content/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /content/t10k-images-idx3-ubyte.gz\n",
            "Extracting /content/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yJwEMGGLpOZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(z,reuse=None):\n",
        "  with tf.variable_scope(\"gen\",reuse=reuse):\n",
        "    hidden1=tf.layers.dense(inputs=z,units=128,activation=tf.nn.leaky_relu)\n",
        "    hidden2=tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n",
        "    output=tf.layers.dense(inputs=hidden2,units=784,activation=tf.nn.tanh)\n",
        "    return output\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9v-55QukZ8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(x,reuse=None):\n",
        "  with tf.variable_scope(\"dis\",reuse=reuse):\n",
        "    hidden1=tf.layers.dense(inputs=x,units=128,activation=tf.nn.leaky_relu)\n",
        "    hidden2=tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n",
        "    logits=tf.layers.dense(inputs=hidden2,units=1)\n",
        "    output=tf.sigmoid(logits)\n",
        "    return output,logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3aY3oYXlfmB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "z=tf.placeholder(tf.float32,shape=[None,100])\n",
        "real_images=tf.placeholder(tf.float32,shape=[None,784])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBToRabpmCH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G=generator(z)\n",
        "\n",
        "real_output,real_logits=discriminator(real_images)\n",
        "fake_output,fake_logits=discriminator(G,reuse=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eMZyQS8ZmrQe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_func(logits_in,labels_in):\n",
        "  return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YKksc2Bh6XSl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_loss_real=loss_func(logits_in=real_logits,labels_in=tf.ones_like(real_logits)*0.9)\n",
        "D_loss_fake=loss_func(logits_in=fake_logits,labels_in=tf.zeros_like(fake_logits))\n",
        "D_loss=D_loss_real+D_loss_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyXES2h_6rUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G_loss=loss_func(logits_in=fake_logits,labels_in=tf.ones_like(fake_logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwjpnmsL8B2T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate=0.001\n",
        "train_vars=tf.trainable_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gLE0ZJt8PFP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d_vars=[x for x in train_vars if \"dis\" in x.name]\n",
        "g_vars=[y for y in train_vars if \"gen\" in y.name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OB6MCTrY8xe7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_trainer=tf.train.AdamOptimizer(learning_rate).minimize(D_loss,var_list=d_vars)\n",
        "G_trainer=tf.train.AdamOptimizer(learning_rate).minimize(G_loss,var_list=g_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQhY2Q1a9ODZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init=tf.global_variables_initializer()\n",
        "num_epochs=500\n",
        "batch_size=100\n",
        "num_batches=mnist.train.num_examples//batch_size\n",
        "samples=[]\n",
        "saver=tf.train.Saver(var_list=g_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2CN8xA9P9jwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8417
        },
        "outputId": "6d2575d1-ab94-42b9-c82d-248669309759"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(num_epochs):\n",
        "    for j in range(num_batches):\n",
        "      batch=mnist.train.next_batch(batch_size)\n",
        "      batch_images=batch[0].reshape([batch_size,784])\n",
        "      batch_images=batch_images*2-1\n",
        "      batch_z=np.random.uniform(-1,1,size=[batch_size,100])\n",
        "      \n",
        "      _=sess.run(D_trainer,feed_dict={z:batch_z,real_images:batch_images})\n",
        "      _=sess.run(G_trainer,feed_dict={z:batch_z})\n",
        "      loss=sess.run(G_loss,feed_dict={z:batch_z})\n",
        "    \n",
        "    \n",
        "    print(\"Currently on step {} , loss={}\".format(i,loss))\n",
        "    sample_z=np.random.uniform(-1,1,size=[1,100])\n",
        "    sample_gen_image=sess.run(generator(z,reuse=True),feed_dict={z:sample_z})\n",
        "    samples.append(sample_gen_image)\n",
        "  saver.save(sess,'/content/500_epochs_model.ckpt')\n",
        "                              \n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently on step 0 , loss=3.505650043487549\n",
            "Currently on step 1 , loss=5.201396465301514\n",
            "Currently on step 2 , loss=1.502576470375061\n",
            "Currently on step 3 , loss=0.7748628854751587\n",
            "Currently on step 4 , loss=4.535179615020752\n",
            "Currently on step 5 , loss=4.224401473999023\n",
            "Currently on step 6 , loss=1.2125742435455322\n",
            "Currently on step 7 , loss=1.6087470054626465\n",
            "Currently on step 8 , loss=6.9824442863464355\n",
            "Currently on step 9 , loss=0.8031566143035889\n",
            "Currently on step 10 , loss=2.4888715744018555\n",
            "Currently on step 11 , loss=0.7284291982650757\n",
            "Currently on step 12 , loss=3.7065865993499756\n",
            "Currently on step 13 , loss=2.029135227203369\n",
            "Currently on step 14 , loss=1.6568520069122314\n",
            "Currently on step 15 , loss=2.2304811477661133\n",
            "Currently on step 16 , loss=1.3792214393615723\n",
            "Currently on step 17 , loss=1.3976856470108032\n",
            "Currently on step 18 , loss=0.9033535718917847\n",
            "Currently on step 19 , loss=2.889540910720825\n",
            "Currently on step 20 , loss=2.0234479904174805\n",
            "Currently on step 21 , loss=1.0613120794296265\n",
            "Currently on step 22 , loss=6.718757152557373\n",
            "Currently on step 23 , loss=2.5906407833099365\n",
            "Currently on step 24 , loss=2.2409822940826416\n",
            "Currently on step 25 , loss=0.983672022819519\n",
            "Currently on step 26 , loss=2.3129096031188965\n",
            "Currently on step 27 , loss=1.341158151626587\n",
            "Currently on step 28 , loss=2.378314971923828\n",
            "Currently on step 29 , loss=2.25813627243042\n",
            "Currently on step 30 , loss=2.0570361614227295\n",
            "Currently on step 31 , loss=1.8589259386062622\n",
            "Currently on step 32 , loss=2.248859167098999\n",
            "Currently on step 33 , loss=2.8520965576171875\n",
            "Currently on step 34 , loss=3.217273235321045\n",
            "Currently on step 35 , loss=2.1703341007232666\n",
            "Currently on step 36 , loss=3.6811435222625732\n",
            "Currently on step 37 , loss=2.423658847808838\n",
            "Currently on step 38 , loss=1.6879816055297852\n",
            "Currently on step 39 , loss=1.6251482963562012\n",
            "Currently on step 40 , loss=2.8991167545318604\n",
            "Currently on step 41 , loss=4.5264668464660645\n",
            "Currently on step 42 , loss=1.90767240524292\n",
            "Currently on step 43 , loss=2.6955392360687256\n",
            "Currently on step 44 , loss=2.872467279434204\n",
            "Currently on step 45 , loss=1.7040127515792847\n",
            "Currently on step 46 , loss=2.7985668182373047\n",
            "Currently on step 47 , loss=2.1937267780303955\n",
            "Currently on step 48 , loss=1.3778339624404907\n",
            "Currently on step 49 , loss=2.900848627090454\n",
            "Currently on step 50 , loss=2.795116662979126\n",
            "Currently on step 51 , loss=2.227231025695801\n",
            "Currently on step 52 , loss=2.304675817489624\n",
            "Currently on step 53 , loss=1.7342207431793213\n",
            "Currently on step 54 , loss=3.088374137878418\n",
            "Currently on step 55 , loss=1.8828010559082031\n",
            "Currently on step 56 , loss=2.4388587474823\n",
            "Currently on step 57 , loss=2.696122646331787\n",
            "Currently on step 58 , loss=2.857661247253418\n",
            "Currently on step 59 , loss=2.205132484436035\n",
            "Currently on step 60 , loss=1.6290907859802246\n",
            "Currently on step 61 , loss=1.120254397392273\n",
            "Currently on step 62 , loss=1.6709685325622559\n",
            "Currently on step 63 , loss=2.7978549003601074\n",
            "Currently on step 64 , loss=1.7666854858398438\n",
            "Currently on step 65 , loss=1.142582654953003\n",
            "Currently on step 66 , loss=1.3708865642547607\n",
            "Currently on step 67 , loss=1.9048550128936768\n",
            "Currently on step 68 , loss=2.8989779949188232\n",
            "Currently on step 69 , loss=1.4726802110671997\n",
            "Currently on step 70 , loss=1.824183464050293\n",
            "Currently on step 71 , loss=1.6805819272994995\n",
            "Currently on step 72 , loss=1.1891653537750244\n",
            "Currently on step 73 , loss=2.0966804027557373\n",
            "Currently on step 74 , loss=1.8346457481384277\n",
            "Currently on step 75 , loss=1.716129183769226\n",
            "Currently on step 76 , loss=1.8230708837509155\n",
            "Currently on step 77 , loss=1.2363471984863281\n",
            "Currently on step 78 , loss=1.8250694274902344\n",
            "Currently on step 79 , loss=1.493412733078003\n",
            "Currently on step 80 , loss=1.6776232719421387\n",
            "Currently on step 81 , loss=1.8426353931427002\n",
            "Currently on step 82 , loss=1.582360029220581\n",
            "Currently on step 83 , loss=1.6800304651260376\n",
            "Currently on step 84 , loss=2.194413661956787\n",
            "Currently on step 85 , loss=0.9786000847816467\n",
            "Currently on step 86 , loss=2.192744731903076\n",
            "Currently on step 87 , loss=1.3459985256195068\n",
            "Currently on step 88 , loss=1.5951788425445557\n",
            "Currently on step 89 , loss=1.6280707120895386\n",
            "Currently on step 90 , loss=1.8693805932998657\n",
            "Currently on step 91 , loss=1.1492981910705566\n",
            "Currently on step 92 , loss=0.9236987829208374\n",
            "Currently on step 93 , loss=1.5540895462036133\n",
            "Currently on step 94 , loss=1.887038230895996\n",
            "Currently on step 95 , loss=1.9749144315719604\n",
            "Currently on step 96 , loss=1.5577294826507568\n",
            "Currently on step 97 , loss=1.3975062370300293\n",
            "Currently on step 98 , loss=1.4433114528656006\n",
            "Currently on step 99 , loss=1.1359941959381104\n",
            "Currently on step 100 , loss=1.6781212091445923\n",
            "Currently on step 101 , loss=1.2669670581817627\n",
            "Currently on step 102 , loss=1.3059049844741821\n",
            "Currently on step 103 , loss=1.4876468181610107\n",
            "Currently on step 104 , loss=1.4103795289993286\n",
            "Currently on step 105 , loss=1.7237253189086914\n",
            "Currently on step 106 , loss=1.478040337562561\n",
            "Currently on step 107 , loss=0.9550531506538391\n",
            "Currently on step 108 , loss=1.076521396636963\n",
            "Currently on step 109 , loss=1.5051847696304321\n",
            "Currently on step 110 , loss=1.6330277919769287\n",
            "Currently on step 111 , loss=1.081648826599121\n",
            "Currently on step 112 , loss=1.199273705482483\n",
            "Currently on step 113 , loss=1.3918269872665405\n",
            "Currently on step 114 , loss=1.5445010662078857\n",
            "Currently on step 115 , loss=1.9182831048965454\n",
            "Currently on step 116 , loss=1.1391973495483398\n",
            "Currently on step 117 , loss=1.1014283895492554\n",
            "Currently on step 118 , loss=1.2531386613845825\n",
            "Currently on step 119 , loss=1.2363249063491821\n",
            "Currently on step 120 , loss=1.3394618034362793\n",
            "Currently on step 121 , loss=1.5785696506500244\n",
            "Currently on step 122 , loss=1.2110599279403687\n",
            "Currently on step 123 , loss=1.3091301918029785\n",
            "Currently on step 124 , loss=1.3147212266921997\n",
            "Currently on step 125 , loss=1.368276834487915\n",
            "Currently on step 126 , loss=1.1579477787017822\n",
            "Currently on step 127 , loss=1.1398407220840454\n",
            "Currently on step 128 , loss=1.2158722877502441\n",
            "Currently on step 129 , loss=1.2892098426818848\n",
            "Currently on step 130 , loss=1.4836435317993164\n",
            "Currently on step 131 , loss=1.3592920303344727\n",
            "Currently on step 132 , loss=1.1675701141357422\n",
            "Currently on step 133 , loss=0.968039870262146\n",
            "Currently on step 134 , loss=1.0128594636917114\n",
            "Currently on step 135 , loss=1.2036960124969482\n",
            "Currently on step 136 , loss=0.8311854004859924\n",
            "Currently on step 137 , loss=1.3377909660339355\n",
            "Currently on step 138 , loss=1.9172558784484863\n",
            "Currently on step 139 , loss=1.0139960050582886\n",
            "Currently on step 140 , loss=1.2699999809265137\n",
            "Currently on step 141 , loss=1.148817777633667\n",
            "Currently on step 142 , loss=1.039084792137146\n",
            "Currently on step 143 , loss=1.0851136445999146\n",
            "Currently on step 144 , loss=0.9779039621353149\n",
            "Currently on step 145 , loss=0.9686176776885986\n",
            "Currently on step 146 , loss=1.4884672164916992\n",
            "Currently on step 147 , loss=1.0406054258346558\n",
            "Currently on step 148 , loss=0.8106657266616821\n",
            "Currently on step 149 , loss=1.2554157972335815\n",
            "Currently on step 150 , loss=1.139051079750061\n",
            "Currently on step 151 , loss=0.7732413411140442\n",
            "Currently on step 152 , loss=1.1646816730499268\n",
            "Currently on step 153 , loss=1.0846552848815918\n",
            "Currently on step 154 , loss=1.0881708860397339\n",
            "Currently on step 155 , loss=1.0492873191833496\n",
            "Currently on step 156 , loss=0.911165714263916\n",
            "Currently on step 157 , loss=1.2353229522705078\n",
            "Currently on step 158 , loss=1.1354588270187378\n",
            "Currently on step 159 , loss=1.0944875478744507\n",
            "Currently on step 160 , loss=0.8945791721343994\n",
            "Currently on step 161 , loss=1.0730469226837158\n",
            "Currently on step 162 , loss=1.2773998975753784\n",
            "Currently on step 163 , loss=1.0428162813186646\n",
            "Currently on step 164 , loss=0.9229689836502075\n",
            "Currently on step 165 , loss=0.9931893348693848\n",
            "Currently on step 166 , loss=1.2608345746994019\n",
            "Currently on step 167 , loss=1.1691404581069946\n",
            "Currently on step 168 , loss=1.0823097229003906\n",
            "Currently on step 169 , loss=1.2649003267288208\n",
            "Currently on step 170 , loss=1.1252208948135376\n",
            "Currently on step 171 , loss=1.1184093952178955\n",
            "Currently on step 172 , loss=1.2536531686782837\n",
            "Currently on step 173 , loss=1.0862125158309937\n",
            "Currently on step 174 , loss=1.1009520292282104\n",
            "Currently on step 175 , loss=1.1298632621765137\n",
            "Currently on step 176 , loss=1.1961774826049805\n",
            "Currently on step 177 , loss=1.1824519634246826\n",
            "Currently on step 178 , loss=0.8212547898292542\n",
            "Currently on step 179 , loss=1.074812650680542\n",
            "Currently on step 180 , loss=1.1239652633666992\n",
            "Currently on step 181 , loss=0.9645633101463318\n",
            "Currently on step 182 , loss=1.192586898803711\n",
            "Currently on step 183 , loss=1.0260170698165894\n",
            "Currently on step 184 , loss=1.3178668022155762\n",
            "Currently on step 185 , loss=1.0599467754364014\n",
            "Currently on step 186 , loss=1.4188618659973145\n",
            "Currently on step 187 , loss=1.070325493812561\n",
            "Currently on step 188 , loss=1.2616018056869507\n",
            "Currently on step 189 , loss=1.1398414373397827\n",
            "Currently on step 190 , loss=1.2348378896713257\n",
            "Currently on step 191 , loss=1.1605688333511353\n",
            "Currently on step 192 , loss=1.1724475622177124\n",
            "Currently on step 193 , loss=1.1339085102081299\n",
            "Currently on step 194 , loss=1.0547301769256592\n",
            "Currently on step 195 , loss=1.0322810411453247\n",
            "Currently on step 196 , loss=1.1543182134628296\n",
            "Currently on step 197 , loss=1.0841091871261597\n",
            "Currently on step 198 , loss=1.3811613321304321\n",
            "Currently on step 199 , loss=1.024458408355713\n",
            "Currently on step 200 , loss=0.9455467462539673\n",
            "Currently on step 201 , loss=1.1325706243515015\n",
            "Currently on step 202 , loss=1.1193293333053589\n",
            "Currently on step 203 , loss=1.1862256526947021\n",
            "Currently on step 204 , loss=0.9783645868301392\n",
            "Currently on step 205 , loss=1.0797126293182373\n",
            "Currently on step 206 , loss=1.1276803016662598\n",
            "Currently on step 207 , loss=0.8840218186378479\n",
            "Currently on step 208 , loss=1.099419355392456\n",
            "Currently on step 209 , loss=1.033856749534607\n",
            "Currently on step 210 , loss=1.2035470008850098\n",
            "Currently on step 211 , loss=1.3077048063278198\n",
            "Currently on step 212 , loss=1.050830364227295\n",
            "Currently on step 213 , loss=1.2458150386810303\n",
            "Currently on step 214 , loss=1.1345343589782715\n",
            "Currently on step 215 , loss=0.9970110058784485\n",
            "Currently on step 216 , loss=1.1825590133666992\n",
            "Currently on step 217 , loss=1.2405203580856323\n",
            "Currently on step 218 , loss=1.0354666709899902\n",
            "Currently on step 219 , loss=1.2783122062683105\n",
            "Currently on step 220 , loss=1.2843366861343384\n",
            "Currently on step 221 , loss=1.0108665227890015\n",
            "Currently on step 222 , loss=1.1555365324020386\n",
            "Currently on step 223 , loss=1.3300862312316895\n",
            "Currently on step 224 , loss=1.1654220819473267\n",
            "Currently on step 225 , loss=1.1296156644821167\n",
            "Currently on step 226 , loss=1.104416847229004\n",
            "Currently on step 227 , loss=1.1879860162734985\n",
            "Currently on step 228 , loss=1.2051576375961304\n",
            "Currently on step 229 , loss=1.0638988018035889\n",
            "Currently on step 230 , loss=1.3505656719207764\n",
            "Currently on step 231 , loss=1.483095645904541\n",
            "Currently on step 232 , loss=1.1058176755905151\n",
            "Currently on step 233 , loss=1.0636920928955078\n",
            "Currently on step 234 , loss=1.0491863489151\n",
            "Currently on step 235 , loss=1.116764783859253\n",
            "Currently on step 236 , loss=1.2732067108154297\n",
            "Currently on step 237 , loss=1.4409499168395996\n",
            "Currently on step 238 , loss=1.1531602144241333\n",
            "Currently on step 239 , loss=1.1926031112670898\n",
            "Currently on step 240 , loss=1.4287233352661133\n",
            "Currently on step 241 , loss=1.41802978515625\n",
            "Currently on step 242 , loss=1.2689300775527954\n",
            "Currently on step 243 , loss=1.222517728805542\n",
            "Currently on step 244 , loss=1.5193105936050415\n",
            "Currently on step 245 , loss=1.4260873794555664\n",
            "Currently on step 246 , loss=1.185556411743164\n",
            "Currently on step 247 , loss=1.2034224271774292\n",
            "Currently on step 248 , loss=1.4064041376113892\n",
            "Currently on step 249 , loss=1.2124098539352417\n",
            "Currently on step 250 , loss=1.2859615087509155\n",
            "Currently on step 251 , loss=1.2252768278121948\n",
            "Currently on step 252 , loss=1.1563431024551392\n",
            "Currently on step 253 , loss=1.3206578493118286\n",
            "Currently on step 254 , loss=1.4217448234558105\n",
            "Currently on step 255 , loss=1.597504734992981\n",
            "Currently on step 256 , loss=1.2886407375335693\n",
            "Currently on step 257 , loss=1.4101693630218506\n",
            "Currently on step 258 , loss=1.2693791389465332\n",
            "Currently on step 259 , loss=1.2196815013885498\n",
            "Currently on step 260 , loss=1.2584867477416992\n",
            "Currently on step 261 , loss=1.076112985610962\n",
            "Currently on step 262 , loss=1.1986687183380127\n",
            "Currently on step 263 , loss=1.5673047304153442\n",
            "Currently on step 264 , loss=1.2180380821228027\n",
            "Currently on step 265 , loss=1.2918225526809692\n",
            "Currently on step 266 , loss=1.2706865072250366\n",
            "Currently on step 267 , loss=1.184648036956787\n",
            "Currently on step 268 , loss=1.2307158708572388\n",
            "Currently on step 269 , loss=1.2574270963668823\n",
            "Currently on step 270 , loss=1.443382978439331\n",
            "Currently on step 271 , loss=1.1050413846969604\n",
            "Currently on step 272 , loss=1.4318767786026\n",
            "Currently on step 273 , loss=1.2201049327850342\n",
            "Currently on step 274 , loss=1.5570889711380005\n",
            "Currently on step 275 , loss=1.275307536125183\n",
            "Currently on step 276 , loss=1.2536109685897827\n",
            "Currently on step 277 , loss=1.4187132120132446\n",
            "Currently on step 278 , loss=1.1348134279251099\n",
            "Currently on step 279 , loss=1.3871785402297974\n",
            "Currently on step 280 , loss=1.2142055034637451\n",
            "Currently on step 281 , loss=1.3325932025909424\n",
            "Currently on step 282 , loss=1.221490502357483\n",
            "Currently on step 283 , loss=1.2170947790145874\n",
            "Currently on step 284 , loss=1.2565860748291016\n",
            "Currently on step 285 , loss=1.3254060745239258\n",
            "Currently on step 286 , loss=1.0209771394729614\n",
            "Currently on step 287 , loss=1.3718287944793701\n",
            "Currently on step 288 , loss=1.324181079864502\n",
            "Currently on step 289 , loss=1.5121984481811523\n",
            "Currently on step 290 , loss=1.509709119796753\n",
            "Currently on step 291 , loss=1.4640065431594849\n",
            "Currently on step 292 , loss=1.253183364868164\n",
            "Currently on step 293 , loss=1.690329909324646\n",
            "Currently on step 294 , loss=1.3976978063583374\n",
            "Currently on step 295 , loss=1.2417148351669312\n",
            "Currently on step 296 , loss=1.389255166053772\n",
            "Currently on step 297 , loss=1.3607964515686035\n",
            "Currently on step 298 , loss=1.1559327840805054\n",
            "Currently on step 299 , loss=1.46042799949646\n",
            "Currently on step 300 , loss=1.1770403385162354\n",
            "Currently on step 301 , loss=1.4010627269744873\n",
            "Currently on step 302 , loss=1.3073310852050781\n",
            "Currently on step 303 , loss=1.3529345989227295\n",
            "Currently on step 304 , loss=1.4156510829925537\n",
            "Currently on step 305 , loss=1.3888177871704102\n",
            "Currently on step 306 , loss=1.5243481397628784\n",
            "Currently on step 307 , loss=1.2032036781311035\n",
            "Currently on step 308 , loss=1.2386798858642578\n",
            "Currently on step 309 , loss=1.4034607410430908\n",
            "Currently on step 310 , loss=1.5063649415969849\n",
            "Currently on step 311 , loss=1.399541974067688\n",
            "Currently on step 312 , loss=1.2192070484161377\n",
            "Currently on step 313 , loss=1.3920265436172485\n",
            "Currently on step 314 , loss=1.2552918195724487\n",
            "Currently on step 315 , loss=1.1708059310913086\n",
            "Currently on step 316 , loss=1.3683042526245117\n",
            "Currently on step 317 , loss=1.4377639293670654\n",
            "Currently on step 318 , loss=1.177506685256958\n",
            "Currently on step 319 , loss=1.3203113079071045\n",
            "Currently on step 320 , loss=1.3039140701293945\n",
            "Currently on step 321 , loss=1.3166868686676025\n",
            "Currently on step 322 , loss=1.3434367179870605\n",
            "Currently on step 323 , loss=1.4366427659988403\n",
            "Currently on step 324 , loss=1.3136461973190308\n",
            "Currently on step 325 , loss=1.3178343772888184\n",
            "Currently on step 326 , loss=1.2626571655273438\n",
            "Currently on step 327 , loss=1.2107092142105103\n",
            "Currently on step 328 , loss=1.457658052444458\n",
            "Currently on step 329 , loss=1.4148386716842651\n",
            "Currently on step 330 , loss=1.408271312713623\n",
            "Currently on step 331 , loss=1.2989088296890259\n",
            "Currently on step 332 , loss=1.3521857261657715\n",
            "Currently on step 333 , loss=1.3498284816741943\n",
            "Currently on step 334 , loss=1.211786150932312\n",
            "Currently on step 335 , loss=1.3383018970489502\n",
            "Currently on step 336 , loss=1.4269288778305054\n",
            "Currently on step 337 , loss=1.296250343322754\n",
            "Currently on step 338 , loss=1.5692639350891113\n",
            "Currently on step 339 , loss=1.3598638772964478\n",
            "Currently on step 340 , loss=1.153303623199463\n",
            "Currently on step 341 , loss=1.4668225049972534\n",
            "Currently on step 342 , loss=1.409888505935669\n",
            "Currently on step 343 , loss=1.2740432024002075\n",
            "Currently on step 344 , loss=1.2982794046401978\n",
            "Currently on step 345 , loss=1.527765154838562\n",
            "Currently on step 346 , loss=1.5865681171417236\n",
            "Currently on step 347 , loss=1.3624218702316284\n",
            "Currently on step 348 , loss=1.6194111108779907\n",
            "Currently on step 349 , loss=1.1122219562530518\n",
            "Currently on step 350 , loss=1.3363144397735596\n",
            "Currently on step 351 , loss=1.5167597532272339\n",
            "Currently on step 352 , loss=1.1244475841522217\n",
            "Currently on step 353 , loss=1.3957709074020386\n",
            "Currently on step 354 , loss=1.4269828796386719\n",
            "Currently on step 355 , loss=1.473820686340332\n",
            "Currently on step 356 , loss=1.305846095085144\n",
            "Currently on step 357 , loss=1.2110618352890015\n",
            "Currently on step 358 , loss=1.3864812850952148\n",
            "Currently on step 359 , loss=1.3497364521026611\n",
            "Currently on step 360 , loss=1.1750879287719727\n",
            "Currently on step 361 , loss=1.3475152254104614\n",
            "Currently on step 362 , loss=1.3057403564453125\n",
            "Currently on step 363 , loss=1.4207680225372314\n",
            "Currently on step 364 , loss=1.506338357925415\n",
            "Currently on step 365 , loss=1.4839532375335693\n",
            "Currently on step 366 , loss=1.5756710767745972\n",
            "Currently on step 367 , loss=1.3443599939346313\n",
            "Currently on step 368 , loss=1.2786685228347778\n",
            "Currently on step 369 , loss=1.44124174118042\n",
            "Currently on step 370 , loss=1.3694179058074951\n",
            "Currently on step 371 , loss=1.1180481910705566\n",
            "Currently on step 372 , loss=1.338506817817688\n",
            "Currently on step 373 , loss=1.424556016921997\n",
            "Currently on step 374 , loss=1.575062870979309\n",
            "Currently on step 375 , loss=1.347181797027588\n",
            "Currently on step 376 , loss=1.4836233854293823\n",
            "Currently on step 377 , loss=1.4505804777145386\n",
            "Currently on step 378 , loss=1.4216636419296265\n",
            "Currently on step 379 , loss=1.2406589984893799\n",
            "Currently on step 380 , loss=1.4398291110992432\n",
            "Currently on step 381 , loss=1.4200412034988403\n",
            "Currently on step 382 , loss=1.2462217807769775\n",
            "Currently on step 383 , loss=1.4804879426956177\n",
            "Currently on step 384 , loss=1.352289080619812\n",
            "Currently on step 385 , loss=1.525193214416504\n",
            "Currently on step 386 , loss=1.4000521898269653\n",
            "Currently on step 387 , loss=1.600264072418213\n",
            "Currently on step 388 , loss=1.3571032285690308\n",
            "Currently on step 389 , loss=1.5894604921340942\n",
            "Currently on step 390 , loss=1.3631300926208496\n",
            "Currently on step 391 , loss=1.5165148973464966\n",
            "Currently on step 392 , loss=1.4351165294647217\n",
            "Currently on step 393 , loss=1.3095479011535645\n",
            "Currently on step 394 , loss=1.432350516319275\n",
            "Currently on step 395 , loss=1.443810224533081\n",
            "Currently on step 396 , loss=1.440203070640564\n",
            "Currently on step 397 , loss=1.4998456239700317\n",
            "Currently on step 398 , loss=1.7855677604675293\n",
            "Currently on step 399 , loss=1.3581138849258423\n",
            "Currently on step 400 , loss=1.2187756299972534\n",
            "Currently on step 401 , loss=1.330450415611267\n",
            "Currently on step 402 , loss=1.439074993133545\n",
            "Currently on step 403 , loss=1.3498754501342773\n",
            "Currently on step 404 , loss=1.3663277626037598\n",
            "Currently on step 405 , loss=1.1843308210372925\n",
            "Currently on step 406 , loss=1.3719371557235718\n",
            "Currently on step 407 , loss=1.4364796876907349\n",
            "Currently on step 408 , loss=1.669643521308899\n",
            "Currently on step 409 , loss=1.2451695203781128\n",
            "Currently on step 410 , loss=1.576967716217041\n",
            "Currently on step 411 , loss=1.2812713384628296\n",
            "Currently on step 412 , loss=1.1690611839294434\n",
            "Currently on step 413 , loss=1.2721799612045288\n",
            "Currently on step 414 , loss=1.3913121223449707\n",
            "Currently on step 415 , loss=1.44024658203125\n",
            "Currently on step 416 , loss=1.3147025108337402\n",
            "Currently on step 417 , loss=1.4445843696594238\n",
            "Currently on step 418 , loss=1.3815388679504395\n",
            "Currently on step 419 , loss=1.5601341724395752\n",
            "Currently on step 420 , loss=1.3284416198730469\n",
            "Currently on step 421 , loss=1.3906314373016357\n",
            "Currently on step 422 , loss=1.3620949983596802\n",
            "Currently on step 423 , loss=1.4992358684539795\n",
            "Currently on step 424 , loss=1.7373254299163818\n",
            "Currently on step 425 , loss=1.430045485496521\n",
            "Currently on step 426 , loss=1.4108145236968994\n",
            "Currently on step 427 , loss=1.161385178565979\n",
            "Currently on step 428 , loss=1.0503413677215576\n",
            "Currently on step 429 , loss=1.3797988891601562\n",
            "Currently on step 430 , loss=1.5732357501983643\n",
            "Currently on step 431 , loss=1.46812105178833\n",
            "Currently on step 432 , loss=1.254692554473877\n",
            "Currently on step 433 , loss=1.6499311923980713\n",
            "Currently on step 434 , loss=1.4023250341415405\n",
            "Currently on step 435 , loss=1.7007505893707275\n",
            "Currently on step 436 , loss=1.3639402389526367\n",
            "Currently on step 437 , loss=1.261905312538147\n",
            "Currently on step 438 , loss=1.343532681465149\n",
            "Currently on step 439 , loss=1.2169562578201294\n",
            "Currently on step 440 , loss=1.399917483329773\n",
            "Currently on step 441 , loss=1.401398777961731\n",
            "Currently on step 442 , loss=1.58046293258667\n",
            "Currently on step 443 , loss=1.3721455335617065\n",
            "Currently on step 444 , loss=1.2738620042800903\n",
            "Currently on step 445 , loss=1.4361191987991333\n",
            "Currently on step 446 , loss=1.7535347938537598\n",
            "Currently on step 447 , loss=1.3616111278533936\n",
            "Currently on step 448 , loss=1.5356539487838745\n",
            "Currently on step 449 , loss=1.5322556495666504\n",
            "Currently on step 450 , loss=1.438576102256775\n",
            "Currently on step 451 , loss=1.3363536596298218\n",
            "Currently on step 452 , loss=1.35795259475708\n",
            "Currently on step 453 , loss=1.4747612476348877\n",
            "Currently on step 454 , loss=1.392990231513977\n",
            "Currently on step 455 , loss=1.6593953371047974\n",
            "Currently on step 456 , loss=1.4473376274108887\n",
            "Currently on step 457 , loss=1.5220425128936768\n",
            "Currently on step 458 , loss=1.2177149057388306\n",
            "Currently on step 459 , loss=1.3627551794052124\n",
            "Currently on step 460 , loss=1.6981197595596313\n",
            "Currently on step 461 , loss=1.4456963539123535\n",
            "Currently on step 462 , loss=1.7985267639160156\n",
            "Currently on step 463 , loss=1.6591827869415283\n",
            "Currently on step 464 , loss=1.5016478300094604\n",
            "Currently on step 465 , loss=1.382952094078064\n",
            "Currently on step 466 , loss=1.3568552732467651\n",
            "Currently on step 467 , loss=1.2209261655807495\n",
            "Currently on step 468 , loss=1.0438312292099\n",
            "Currently on step 469 , loss=1.2600808143615723\n",
            "Currently on step 470 , loss=1.4725090265274048\n",
            "Currently on step 471 , loss=1.3231106996536255\n",
            "Currently on step 472 , loss=1.260572910308838\n",
            "Currently on step 473 , loss=1.606940746307373\n",
            "Currently on step 474 , loss=1.350606083869934\n",
            "Currently on step 475 , loss=1.5369415283203125\n",
            "Currently on step 476 , loss=1.364195704460144\n",
            "Currently on step 477 , loss=1.5850619077682495\n",
            "Currently on step 478 , loss=1.4804067611694336\n",
            "Currently on step 479 , loss=1.3321864604949951\n",
            "Currently on step 480 , loss=1.3910794258117676\n",
            "Currently on step 481 , loss=1.5095640420913696\n",
            "Currently on step 482 , loss=1.6292251348495483\n",
            "Currently on step 483 , loss=1.4660638570785522\n",
            "Currently on step 484 , loss=1.3396594524383545\n",
            "Currently on step 485 , loss=1.397458791732788\n",
            "Currently on step 486 , loss=1.2475847005844116\n",
            "Currently on step 487 , loss=1.4429028034210205\n",
            "Currently on step 488 , loss=1.5717579126358032\n",
            "Currently on step 489 , loss=1.4501068592071533\n",
            "Currently on step 490 , loss=1.2936924695968628\n",
            "Currently on step 491 , loss=1.3480043411254883\n",
            "Currently on step 492 , loss=1.374685525894165\n",
            "Currently on step 493 , loss=1.5446648597717285\n",
            "Currently on step 494 , loss=1.2879836559295654\n",
            "Currently on step 495 , loss=1.6061046123504639\n",
            "Currently on step 496 , loss=1.2661206722259521\n",
            "Currently on step 497 , loss=1.4416955709457397\n",
            "Currently on step 498 , loss=1.3942809104919434\n",
            "Currently on step 499 , loss=1.4290499687194824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jxWCkOLI9qCu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#plt.imshow(samples[99].reshape(28,28),cmap=\"Greys\")\n",
        "new_samples=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kamsRZqeM4hT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b70fbe5-adb9-4e06-8a80-18358e9fa0d7"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  saver.restore(sess,'/content/500_epochs_model.ckpt')\n",
        "  for x in range(50):\n",
        "    sample_z=np.random.uniform(-1,1,size=[1,100])\n",
        "    gen_sample=sess.run(generator(z,reuse=True),feed_dict={z:sample_z})\n",
        "    new_samples.append(gen_sample)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/500_epochs_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KC9lFGGWQFFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8d578e5e-6a50-442c-9ebc-50f215d3a2c1"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(new_samples[34].reshape(28,28),cmap=\"Greys\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f569c720d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f569c8bdc88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "aemtWQUtQFzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}